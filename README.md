1. Project ObjectiveThe business goal is to develop an intelligent system that can decide whether to approve or deny a new loan application to maximize the company's financial return. This project moves beyond simple risk prediction (a supervised learning task) to learn an optimal decision-making policy (an offline reinforcement learning task) and critically compares their performance.2. Key FindingsThe final analysis, conducted on a held-out test set of 202,215 loans, revealed a clear winner:Baseline "Approve All" Policy: This policy is catastrophically unprofitable, resulting in a net loss of -$334.2M (or -$1652.71 per loan).Model 1 (DL Risk Model): A simple, risk-averse policy based on our PyTorch MLP was highly effective. It improved the bottom line by over **$300M**, resulting in a loss of only **-$30.4M ** (or  -$150.36\ per loan).Model 2 (RL Profit Model): The RL agent (DiscreteCQL) successfully learned from the biased data and improved on the baseline, but its "profit-seeking" policy was flawed.The default agent (alpha=1.0) was "baited" by high-reward loans and lost -$172.3M.The tuned agent (alpha=5.0) was more conservative and performed better, but still lost -$80.0M, failing to beat the simpler DL model.Conclusion: The simple, risk-based DL model was the superior and most robust solution. The RL agent's failure to outperform it was a direct result of the extreme data bias in the offline dataset.3. DatasetThis project uses the LendingClub Loan Data dataset, available on Kaggle.File: accepted_2007_to_2018.csvDownload: Kaggle: LendingClub Loan Data4. MethodologyThe project is broken into four distinct tasks, all contained within the main Jupyter Notebook:Task 1: EDA & Preprocessing:Filtered 2.26M+ loans down to 1.35M with terminal outcomes ("Fully Paid" or "Charged Off").Engineered the binary target variable (for DL) and the financial reward variable (for RL).Performed deep EDA to find predictive features (e.g., grade, int_rate, fico_range_low)3.Built a scikit-learn ColumnTransformer pipeline to impute, encode, and scale 19 features into a final 40-dimension vector.Task 2: Model 1 - Predictive DL Model (PyTorch):Built an MLP to predict the probability of default.Handled the 4:1 class imbalance using a pos_weight in the loss function.Used Early Stopping to find and save the best-performing model (dl_model.pt).Task 3: Model 2 - Offline RL Agent (d3rlpy):Framed the problem as an MDP (State, Action, Reward).Trained a DiscreteCQL agent, which is designed for biased, offline datasets.Iteration: Trained two agents to compare: one with default conservatism (alpha=1.0) and one tuned agent (alpha=5.0).Task 4: Analysis & Comparison:Loaded the test set and all saved models (dl_model.pt, cql_policy.pt, cql_policy_alpha_5.pt).Calculated the Estimated Policy Value (total financial return) for all policies.Analyzed the disagreements to understand why the DL model outperformed the RL agent.5. Setup & InstallationTo reproduce this analysis, please follow these setup instructions.Clone the repository:Bashgit clone <your-repo-url>
cd <your-repo-name>
Create and activate a virtual environment:Bashpython -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
Create a requirements.txt file with the following content:Plaintextpandas
numpy
scikit-learn
matplotlib
seaborn
torch
d3rlpy
joblib
gymnasium
jupyter
Install the dependencies:Bashpip install -r requirements.txt
6. How to RunAll code is contained in the Jupyter Notebook notebook564f336f4d.ipynb4.Download the data (accepted_2007_to_2018.csv) and place it in the path specified in Cell 3 of the notebook (e.g., /kaggle/input/accepted11/). You may need to create this directory structure or update the path.Run the notebook cells sequentially. The notebook is organized by task:Cell 1-2: Imports and initial data path setup.Cell 3: Task 1 (Part 1) - Loads data, engineers target, and performs deep EDA.Cell 4: Task 1 (Part 2) - Runs the scikit-learn preprocessing pipeline and saves all processed data (X_train.npy, y_train.csv, r_train.csv, etc.) to /kaggle/working/ (or the notebook's root directory).Cell 5: Task 2 - Trains the DL (PyTorch) model with Early Stopping. Saves the best model as dl_model.pt and reports its test set AUC/F1 scores.Cell 7: Installs the d3rlpy library.Cell 9: Task 3 (Base) - Trains the default DiscreteCQL agent (alpha=1.0). Saves the policy as cql_policy.pt.Cell 11: Task 3 (Tuned) - Trains the upgraded DiscreteCQL agent (alpha=5.0). Saves the policy as cql_policy_alpha_5.pt.Cell 12: Task 4 (Analysis) - Loads all three saved models (dl_model.pt, cql_policy.pt, cql_policy_alpha_5.pt), runs them on the test set, and prints the final Key Results table and Disagreement Analysis.7. Final Results SummaryThis is the final output from Cell 12, comparing the financial performance of all policies on the 202,215-loan test set5.Policy / ModelPrimary GoalKey Metrics (Accuracy)Approval RateEstimated Policy Value (Total)Simulated Avg. Return (per loan)Baseline (Approve All)N/AN/A100.0%-$334,202,183.47-$1652.71Model 1: DL (Task 2)Maximize F1/AUCAUC: 0.7187, F1: 0.433657.8%-$30,405,388.09-$150.36Model 2: RL (alpha=1.0)Maximize ReturnN/A53.9%-$172,261,179.99-$851.86Model 2: RL (alpha=5.0)Maximize ReturnN/A31.9%-$80,036,937.55-$395.808. Analysis & ConclusionThe simple DL risk model (Task 2) was the clear winner, providing the best financial outcome by a margin of $50M over the tuned RL agent.The RL agent (Task 3) learned a flawed "barbell" policy:It was too pessimistic: It denied 70,810 safe, low-interest loans that the DL model correctly approved, sacrificing profit.It was too optimistic: It was "baited" by high interest rates, approving 18,636 high-risk loans that the DL model correctly denied, leading to massive default losses.This behavior is a classic symptom of training on a highly biased dataset (which only contained "Approve" actions). While tuning alpha from 1.0 to 5.0 made the agent more conservative and improved its return by $92M, it was not enough to overcome the data bias.Future work would focus on acquiring "counterfactual" data on rejected applicants and re-framing the problem for the RL agent to set the interest rate itself, rather than just making a "Deny/Approve" decision
